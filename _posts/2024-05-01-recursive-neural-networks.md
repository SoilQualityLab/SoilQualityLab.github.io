---
layout: post
title:  "Learning From Soil: Executable List APIs are smarter RNNs"
date:   2024-05-01 4:30:00
categories: template
---

## Soil ecoystem modeling is fundmentally about infinitely recursive neural networks

**Sequester C as life** is about why brains are carbon-based, rather than silicon-based ... understand the reason for why Life is based upon the carbon cycle RATHER than just *pounding on about sand* involves [in part] intuitively understand how neurons have evolved over millions of years and now develop morphologically [and sustainably].

If we want to model soil ecosystems we probably need to look at the models for how our brains work ... we know, or at least some of us actually understand, how our brains are different than the simplistic algorithms known as *neural networks* ... but [recursive neural networks are reasonably effective](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) approximation for the GENERAL process of how our brains use ATTENTION to ***process*** and re-process, encode, decode, backward/forword propagate, endlessly ruminate on *prior* information ... when it comes to thinking about thinking, the conventional wisdom can sort of be summarized by the sentence, ["We are all Bayesians now"](https://towardsdatascience.com/are-we-all-bayesian-our-brains-think-so-555cedaffed9), ie generally speak, we all kind of *GET* or at least believe that *pretty much almost EVERY last poriton of a thought that runs through our heads is a product of the long-term cultural accumulation of what we, all of those in contact with us and everyone who's has ever been in contact with or somehow influenced those people has thought before.*

This is why ORIGINAL thought ... and all of those people who tend to think originally or *way outside of the box* ... tends to scare the living shit out of *normal* human beings.

## What's an RNN?

*Funny* that you should ask ... well, it's not that *funny* ... but we could probably come us with an NLP algorithm that would would be a LOT funnier *[and less nasueatingly predictable]* than something like SNL or some talkshow host or podcaster you listen to. Actually, it is pretty safe to say that a lot of people are working pretty hard on a way to [BE FUNNIER](https://www.Funnier.Be).

Actually, LOTS of people have been asking about RNNs in the last five years or so ... there are even books about RNNs that actually SELL to sort of mainstream readers; there are quasi-*ordinaryish* people paying attention this topic now ... and more will pay *attention* to it in the future ... in general, the answer to the questions about ***recursive*** *neural networks* is, "It all depends on what you mean by [**transform***ative experience](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)*."